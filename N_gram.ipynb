{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD9ZRNeIJ_Br"
      },
      "source": [
        "# Download and Process the IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "siDkX864cazJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "from math import log, exp\n",
        "\n",
        "n = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G1K-8ybJJ3Cl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_imdb_unsup_sentences(folder_path):\n",
        "    \"\"\"\n",
        "    Loads text files from the IMDB 'unsup' (unsupervised) folder.\n",
        "    split text by newline, strips text, and returns a list of raw lines.\n",
        "    replace <br /> tags with special token <nl> token.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        text = open(os.path.join(folder_path,file),\"r\",encoding=\"utf-8\")\n",
        "        for line in text.readlines():\n",
        "            all_sentences.append(line.replace(\"<br />\",\" <nl> \").strip())\n",
        "         \n",
        "    return all_sentences\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"\n",
        "    Removes punctuation from the text,\n",
        "    but keeps <nl> tokens intact.\n",
        "    \"\"\"\n",
        "    text = text.replace(\" <nl> \", \" xnl \")  # Temporarily separate <nl> to preserve it\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.replace(\" xnl \", \" <nl> \")  # Restore <nl> in its place\n",
        "    return text\n",
        "\n",
        "def build_vocabulary(sentences):\n",
        "    \"\"\"\n",
        "    lower each sentence,\n",
        "    Splits each sentence on whitespace, removes punctuation,\n",
        "    and builds a set of unique tokens (vocabulary).\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    sentence:str\n",
        "    for sentence in sentences:\n",
        "        sentence = remove_punctuation(sentence).lower()\n",
        "        for word in sentence.split():\n",
        "            if word != \"<nl>\":\n",
        "                # print(\"fdssfsdfsdfsdfsdf\")\n",
        "                vocab.add(word)\n",
        "        \n",
        "    return vocab\n",
        "\n",
        "def tokinize(sentences, vocab, unknown=\"<UNK>\"):\n",
        "    \"\"\"\n",
        "    lower each sentence,\n",
        "    Splits each sentence on whitespace, removes punctuation,\n",
        "    and replaces tokens not in the vocabulary with unknowen token.\n",
        "    Returns the list of tokenized sentences.\n",
        "    \"\"\"\n",
        "    tokenized_sentences = []\n",
        "    sentence:str\n",
        "    for sentence in sentences:\n",
        "        sentence = remove_punctuation(sentence).lower()\n",
        "        for word in sentence.split():\n",
        "            if word not in vocab and word != \"<nl>\":\n",
        "                sentence.replace(word,unknown)\n",
        "        tokenized_sentences.append(sentence.split())\n",
        "\n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-5469mMKcRP",
        "outputId": "736ae7a1-6af4-409e-bb63-6f584154654e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of raw sentences loaded: 50000\n",
            "Example (first 2 sentences):\n",
            "['I admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931). <nl>  <nl> So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film. <nl>  <nl> One intriguing sequence. Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?']\n"
          ]
        }
      ],
      "source": [
        "#imdb_folder = \"imdb_data/train/unsup\"\n",
        "imdb_folder = \"S:/imdb_data/train/unsup\"\n",
        "\n",
        "sentences = load_imdb_unsup_sentences(imdb_folder)\n",
        "\n",
        "print(f\"Number of raw sentences loaded: {len(sentences)}\")\n",
        "print(f\"Example (first 2 sentences):\\n{sentences[:1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Qv0J6dGhIidP"
      },
      "outputs": [],
      "source": [
        "assert len(sentences) == 50000, \"Expected 50,000 sentences from the unsup folder.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9hA3B8WEKAF_"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(sentences, test_split=0.1):\n",
        "    \"\"\"\n",
        "      shuffle the sentences\n",
        "      split them into train and test sets (first 1-test_split of the data is the training)\n",
        "      return the train and test sets\n",
        "    \"\"\"\n",
        "    random.shuffle(sentences)\n",
        "    train_split_length = int(len(sentences)*(1-test_split))\n",
        "    train_sentences, test_sentences = sentences[:train_split_length],sentences[train_split_length:]\n",
        "    \n",
        "    return train_sentences, test_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfYWg45aKNzP",
        "outputId": "5c2b987e-6566-4dcb-9d52-e9654148797e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training sentences: 45000\n",
            "Number of test sentences: 5000\n"
          ]
        }
      ],
      "source": [
        "train_sentences, test_sentences = split_data(sentences)\n",
        "\n",
        "print(f\"Number of training sentences: {len(train_sentences)}\")\n",
        "print(f\"Number of test sentences: {len(test_sentences)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "i9Hh9ptkKS6Y"
      },
      "outputs": [],
      "source": [
        "assert len(train_sentences) == 45000, \"Expected 45,000 sentences for training.\"\n",
        "assert len(test_sentences) == 5000, \"Expected 5,000 sentences for testing.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_q9qARKw_u",
        "outputId": "3ce46403-5f41-464d-9907-9b1bd9de6bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 161292\n",
            "Example tokens from first sentence: ['having', 'first', 'seen', 'the', 'directors', '12min', 'take', 'on', 'poes', 'fall', 'of', 'the', 'house', 'of', 'usher', 'i', 'was', 'looking', 'forward', 'to', 'seeing', 'this', 'one', 'too', 'and', 'wasnt', 'disappointed', 'at', 'all', 'though', 'perhaps', 'not', 'quite', 'up', 'to', 'the', 'same', 'level', 'of', 'artistic', 'attainment', 'as', 'usher', 'it', 'is', 'nevertheless', 'very', 'much', 'in', 'the', 'same', 'vein', '<nl>', '<nl>', 'like', 'the', 'usher', 'the', 'viewer', 'should', 'be', 'familiar', 'beforehand', 'with', 'the', 'story', 'on', 'which', 'it', 'is', 'based', 'in', '1928', 'the', 'directors', 'watson', 'and', 'webber', 'could', 'have', 'safely', 'assumed', 'the', 'audiences', 'knowledge', 'of', 'the', 'biblical', 'tale', 'interestingly'] ...\n"
          ]
        }
      ],
      "source": [
        "vocab = build_vocabulary(train_sentences)\n",
        "tokenized_sentences = tokinize(train_sentences, vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example tokens from first sentence: {tokenized_sentences[0][:90] if tokenized_sentences else 'No tokens loaded'} ...\")\n",
        "\n",
        "# for word in vocab:\n",
        "#     if word == \"nl\":\n",
        "#         print(\"fdssfsdfsdfsdfsdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9lbynIF5K6xJ"
      },
      "outputs": [],
      "source": [
        "assert len(vocab) == 161292, \"Expected a vocabulary size of 171,591.\"\n",
        "assert len(tokenized_sentences) == 45000, \"Expected tokenized sentences count to match raw sentences.\"\n",
        "\n",
        "example = \"I love Natural language processing, and i want to be a great engineer.\"\n",
        "assert len(example) == 70, \"Example sentence length (in characters) does not match the expected 70.\"\n",
        "\n",
        "example_tokens = tokinize([example], vocab)[0]\n",
        "\n",
        "assert len(example_tokens) == 13, \"Token count for the example sentence does not match the expected 13.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9bQ5K2ubNFhD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pad_sentence(tokens, n):\n",
        "    \"\"\"\n",
        "    Pads a list of tokens with <s> at the start (n-1 times)\n",
        "    and </s> at the end (once).\n",
        "    For example, if n=3, you add 2 <s> tokens at the start.\n",
        "    \"\"\"\n",
        "    #! Probably it should return a list not a string.\n",
        "    #! The provided code returns a string\n",
        "    padded = [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
        "    return padded\n",
        "\n",
        "def build_ngram_counts(tokenized_sentences, n,vocab):\n",
        "    \"\"\"\n",
        "    Builds n-gram counts and (n-1)-gram counts from the given tokenized sentences.\n",
        "    Each sentence is padded with <s> and </s>.\n",
        "\n",
        "    Args:\n",
        "        tokenized_sentences: list of lists, where each sub-list is a tokenized sentence.\n",
        "        n: the order of the n-gram (e.g., 2 for bigrams, 3 for trigrams).\n",
        "        vocab: set of known words. If provided, you can choose to handle out-of-vocab tokens.\n",
        "\n",
        "    Returns:\n",
        "        ngram_counts: Counter of n-grams (tuples of length n).\n",
        "        context_counts: Counter of (n-1)-gram contexts.\n",
        "    \"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence,n)\n",
        "        \n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i:i + n])  \n",
        "            context = tuple(padded_sentence[i:i + n - 1]) \n",
        "            \n",
        "\n",
        "            context_counts[context] += 1\n",
        "            \n",
        "            if \"<nl><nl>\" in \"\".join(ngram):  \n",
        "                continue\n",
        "            # if \"<nl>\" in context:\n",
        "            #     continue\n",
        "\n",
        "            ngram_counts[ngram] += 1\n",
        "            \n",
        "    \n",
        "        \n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "def laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the probability of an n-gram using Laplace (add-alpha) smoothing.\n",
        "\n",
        "    P(w_i | w_{i-(n-1)}, ..., w_{i-1}) =\n",
        "        (count(ngram) + alpha) / (count(context) + alpha * vocab_size)\n",
        "\n",
        "    Args:\n",
        "        ngram: tuple of tokens representing the n-gram\n",
        "        ngram_counts: Counter of n-grams\n",
        "        context_counts: Counter of (n-1)-gram contexts\n",
        "        vocab_size: size of the vocabulary\n",
        "        alpha: smoothing parameter (1.0 = add-1 smoothing)\n",
        "\n",
        "    Returns:\n",
        "        Probability of the given n-gram.\n",
        "    \"\"\"\n",
        "    ngram_count = ngram_counts.get(ngram, 0) \n",
        "    context = ngram[:-1]\n",
        "    context_count = context_counts.get(context, 0) \n",
        "\n",
        "\n",
        "    prob = (ngram_count + alpha) / (context_count + (alpha * context_count) + 10e-10)\n",
        "\n",
        "\n",
        "    return prob\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFRligyRx_8",
        "outputId": "0e6133ac-7d80-4597-a086-b85934d64416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of bigrams: 6090230\n",
            "Number of contexts: 2249327\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n, vocab=vocab)\n",
        "context_counts = {context: count for context, count in context_counts.items() if \"<nl>\" not in context}\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example bigrams: [(('<s>', '<s>', 'having'), 137), (('<s>', 'having', 'first'), 1), (('having', 'first', 'seen'), 3), (('first', 'seen', 'the'), 2), (('seen', 'the', 'directors'), 1), (('the', 'directors', '12min'), 1), (('directors', '12min', 'take'), 1), (('12min', 'take', 'on'), 1), (('take', 'on', 'poes'), 1), (('on', 'poes', 'fall'), 1), (('poes', 'fall', 'of'), 1), (('fall', 'of', 'the'), 26), (('of', 'the', 'house'), 120), (('the', 'house', 'of'), 74), (('house', 'of', 'usher'), 7), (('of', 'usher', 'i'), 1), (('usher', 'i', 'was'), 1), (('i', 'was', 'looking'), 116), (('was', 'looking', 'forward'), 74), (('looking', 'forward', 'to'), 275), (('forward', 'to', 'seeing'), 118), (('to', 'seeing', 'this'), 38), (('seeing', 'this', 'one'), 15), (('this', 'one', 'too'), 24), (('one', 'too', 'and'), 4), (('too', 'and', 'wasnt'), 1), (('and', 'wasnt', 'disappointed'), 5), (('wasnt', 'disappointed', 'at'), 2), (('disappointed', 'at', 'all'), 5), (('at', 'all', 'though'), 9), (('all', 'though', 'perhaps'), 1), (('though', 'perhaps', 'not'), 3), (('perhaps', 'not', 'quite'), 4), (('not', 'quite', 'up'), 11), (('quite', 'up', 'to'), 18), (('up', 'to', 'the'), 553), (('to', 'the', 'same'), 110), (('the', 'same', 'level'), 61), (('same', 'level', 'of'), 26), (('level', 'of', 'artistic'), 1), (('of', 'artistic', 'attainment'), 1), (('artistic', 'attainment', 'as'), 1), (('attainment', 'as', 'usher'), 1), (('as', 'usher', 'it'), 1), (('usher', 'it', 'is'), 1), (('it', 'is', 'nevertheless'), 10), (('is', 'nevertheless', 'very'), 2), (('nevertheless', 'very', 'much'), 1), (('very', 'much', 'in'), 45), (('much', 'in', 'the'), 110), (('in', 'the', 'same'), 688), (('the', 'same', 'vein'), 38), (('same', 'vein', '<nl>'), 1), (('<nl>', 'like', 'the'), 32), (('like', 'the', 'usher'), 1), (('the', 'usher', 'the'), 1), (('usher', 'the', 'viewer'), 1), (('the', 'viewer', 'should'), 6), (('viewer', 'should', 'be'), 4), (('should', 'be', 'familiar'), 3), (('be', 'familiar', 'beforehand'), 1), (('familiar', 'beforehand', 'with'), 1), (('beforehand', 'with', 'the'), 1), (('with', 'the', 'story'), 171), (('the', 'story', 'on'), 20), (('story', 'on', 'which'), 3), (('on', 'which', 'it'), 23), (('which', 'it', 'is'), 75), (('it', 'is', 'based'), 104), (('is', 'based', 'in'), 12), (('based', 'in', '1928'), 1), (('in', '1928', 'the'), 1), (('1928', 'the', 'directors'), 1), (('the', 'directors', 'watson'), 1), (('directors', 'watson', 'and'), 1), (('watson', 'and', 'webber'), 1), (('and', 'webber', 'could'), 1), (('webber', 'could', 'have'), 1), (('could', 'have', 'safely'), 1), (('have', 'safely', 'assumed'), 1), (('safely', 'assumed', 'the'), 1), (('assumed', 'the', 'audiences'), 1), (('the', 'audiences', 'knowledge'), 1), (('audiences', 'knowledge', 'of'), 1), (('knowledge', 'of', 'the'), 71), (('of', 'the', 'biblical'), 6), (('the', 'biblical', 'tale'), 3), (('biblical', 'tale', 'interestingly'), 1), (('tale', 'interestingly', 'apart'), 1), (('interestingly', 'apart', 'from'), 1), (('apart', 'from', 'the'), 187), (('from', 'the', 'actual'), 17), (('the', 'actual', 'genesis'), 1), (('actual', 'genesis', 'account'), 1), (('genesis', 'account', 'a'), 1), (('account', 'a', 'phrase'), 1), (('a', 'phrase', 'from'), 2), (('phrase', 'from', 'the'), 3), (('from', 'the', 'song'), 4), (('the', 'song', 'of'), 8), (('song', 'of', 'songs'), 1), (('of', 'songs', 'is'), 1), (('songs', 'is', 'also'), 2), (('is', 'also', 'used'), 7), (('also', 'used', 'when'), 1), (('used', 'when', 'lot'), 1), (('when', 'lot', 'is'), 1), (('lot', 'is', 'offering'), 1), (('is', 'offering', 'his'), 1), (('offering', 'his', 'daughters'), 1), (('his', 'daughters', 'to'), 1), (('daughters', 'to', 'the'), 4), (('to', 'the', 'mob'), 10), (('the', 'mob', 'outside'), 1), (('mob', 'outside', 'desperately'), 1), (('outside', 'desperately', 'trying'), 1), (('desperately', 'trying', 'to'), 36), (('trying', 'to', 'convince'), 32), (('to', 'convince', 'them'), 6), (('convince', 'them', 'of'), 1), (('them', 'of', 'the'), 11), (('of', 'the', 'attractions'), 6), (('the', 'attractions', 'of'), 5), (('attractions', 'of', 'woman'), 1), (('of', 'woman', 'to'), 2), (('woman', 'to', 'complain'), 1), (('to', 'complain', 'that'), 4), (('complain', 'that', 'the'), 9), (('that', 'the', 'film'), 618), (('the', 'film', 'does'), 227), (('film', 'does', 'not'), 103), (('does', 'not', 'present'), 2), (('not', 'present', 'the'), 1), (('present', 'the', 'plot'), 1), (('the', 'plot', 'more'), 9), (('plot', 'more', 'overtly'), 1), (('more', 'overtly', 'is'), 1), (('overtly', 'is', 'beside'), 1), (('is', 'beside', 'the'), 5), (('beside', 'the', 'point'), 18), (('the', 'point', 'and'), 32), (('point', 'and', 'almost'), 2), (('and', 'almost', 'a'), 2), (('almost', 'a', 'declaration'), 1), (('a', 'declaration', 'of'), 2), (('declaration', 'of', 'ignorance'), 1), (('of', 'ignorance', '<nl>'), 1), (('<nl>', 'the', 'basics'), 1), (('the', 'basics', 'of'), 11), (('basics', 'of', 'this'), 1)]\n",
            "Example contexts: [(('<s>', '<s>'), 45000), (('<s>', 'having'), 137), (('having', 'first'), 4), (('first', 'seen'), 29), (('seen', 'the'), 1083), (('the', 'directors'), 561), (('directors', '12min'), 1), (('12min', 'take'), 1), (('take', 'on'), 355), (('on', 'poes'), 4), (('poes', 'fall'), 1), (('fall', 'of'), 82), (('of', 'the'), 70204), (('the', 'house'), 828), (('house', 'of'), 243)]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Example bigrams: {list(ngram_counts.items())[:150]}\")\n",
        "print(f\"Example contexts: {list(context_counts.items())[:15]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v4NzCsBMzTN",
        "outputId": "2aa3c438-e7de-457b-ef1e-f19ae1cd2b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Sequence: ['i', 'love', 'the', 'way', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very', 'good', 'and', 'the', 'film', 'is', 'a', 'very']\n"
          ]
        }
      ],
      "source": [
        "from math import log, exp\n",
        "\n",
        "def predict_next_token(\n",
        "    context_tokens,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab,\n",
        "    n,\n",
        "    alpha=1.0,\n",
        "    top_k=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Given a list of context tokens, predict the next token using the n-gram model.\n",
        "    Returns the top_k predictions as (token, probability).\n",
        "    \"\"\"\n",
        "\n",
        "    candidates = []\n",
        "    context = tuple(context_tokens[-(n - 1):])  # Extract the last (n-1) tokens as context\n",
        "    \n",
        "    vocab_size = len(vocab)\n",
        "    \n",
        "    for token in vocab:\n",
        "        ngram = context + (token,)  # Create an n-gram candidate\n",
        "        prob = laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha)\n",
        "        candidates.append((token, prob))\n",
        "    \n",
        "    # Sort by probability in descending order and return top_k results\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:top_k]\n",
        "\n",
        "\n",
        "def generate_text_with_limit(\n",
        "    start_tokens,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab,\n",
        "    n,\n",
        "    alpha=1.0,\n",
        "    max_length=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates text from an n-gram model until it sees </s>\n",
        "    or reaches a maximum total length (max_length).\n",
        "\n",
        "    Args:\n",
        "      start_tokens (list): initial context to begin generation\n",
        "      ngram_counts (Counter): trained n-gram counts\n",
        "      context_counts (Counter): trained (n-1)-gram counts\n",
        "      vocab (set): the model vocabulary\n",
        "      n (int): n-gram order, 2 for bigram, 3 for trigram, etc.\n",
        "      alpha (float): Laplace smoothing parameter\n",
        "      max_length (int): maximum number of tokens to generate (including start_tokens)\n",
        "\n",
        "    Returns:\n",
        "      A list of tokens representing the generated sequence.\n",
        "    \"\"\"\n",
        "    generated = list(start_tokens)  # Initialize with given start tokens\n",
        "    \n",
        "    while len(generated) < max_length:\n",
        "        predictions = predict_next_token(generated, ngram_counts, context_counts, vocab, n, alpha)\n",
        "        \n",
        "        next_token = predictions[0][0]  # Select the most probable token\n",
        "        \n",
        "        if next_token == \"</s>\": \n",
        "            break\n",
        "        \n",
        "        generated.append(next_token)  \n",
        "    \n",
        "    return generated\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=1.0,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_LN_xGAcGPnt"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(\n",
        "    tokenized_sentences,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab_size,\n",
        "    n,\n",
        "    alpha=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of an n-gram model (with Laplace smoothing)\n",
        "    on a list of tokenized sentences.\n",
        "\n",
        "    Args:\n",
        "      tokenized_sentences: List of lists of tokens.\n",
        "      ngram_counts: Counter of n-grams.\n",
        "      context_counts: Counter of (n-1)-grams.\n",
        "      vocab_size: Size of the vocabulary.\n",
        "      n: n-gram order.\n",
        "      alpha: Laplace smoothing parameter.\n",
        "\n",
        "    Returns:\n",
        "      A float representing the perplexity on the given dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    log_prob_sum = 0.0\n",
        "    word_count = 0  \n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i:i + n])\n",
        "            prob = laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha)\n",
        "\n",
        "            log_prob_sum += log(prob)  \n",
        "            word_count += 1\n",
        "\n",
        "    perplexity = exp((-1/word_count)*log_prob_sum ) if word_count > 0 else float('inf')\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ExBYFpgj30"
      },
      "source": [
        "# **Analysis**\n",
        "use different n and rerun the code and write down your analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity for n=1: 2416.2647330664254\n",
            "Perplexity for n=2: 244.05077090940617\n",
            "Perplexity for n=3: 1.0592658345689983\n"
          ]
        }
      ],
      "source": [
        "n = range(1,4) # N from 1 to 3\n",
        "tokenized_test_sentences = tokinize(test_sentences, vocab)\n",
        "for i in n:\n",
        "    ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, i, vocab=vocab)\n",
        "    padded_test_sentences = [pad_sentence(sentence, i) for sentence in tokenized_sentences]\n",
        "    pp= calculate_perplexity(tokenized_test_sentences, ngram_counts, context_counts, len(vocab), i, alpha=1.0)\n",
        "    print(f\"Perplexity for n={i}: {pp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using NLTK Library to evaluate the preplexity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Laplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Model Perplexity with n=1 : 958.6964613574661\n",
            "NLTK Model Perplexity with n=2 : 1661.488686634242\n",
            "NLTK Model Perplexity with n=3 : 4194.543335552027\n"
          ]
        }
      ],
      "source": [
        "for i in n:\n",
        "    ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, i, vocab=vocab)\n",
        "\n",
        "    nltk_train_data, nltk_vocab = padded_everygram_pipeline(i, tokenized_sentences)\n",
        "    nltk_model = Laplace(i) \n",
        "    nltk_model.fit(nltk_train_data, nltk_vocab)\n",
        "    \n",
        "    nltk_test_data, _ = padded_everygram_pipeline(i, tokenized_test_sentences)\n",
        "    nltk_perplexity = nltk_model.perplexity(next(nltk_test_data))\n",
        "    print(f\"NLTK Model Perplexity with n={i}: {nltk_perplexity}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes on the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why Perplexity Increases with n:\n",
        "\n",
        "- Data Sparsity:\n",
        "As n increases, the number of possible n-grams grows exponentially.\n",
        "Many n-grams in the test data may not have been seen in the training data, leading to more cases where probabilities are extremely low or zero (even with smoothing).\n",
        "- Context Specificity:\n",
        "Larger n means the model relies on longer context windows.\n",
        "If an (n-1)-gram context was rare in training, then the corresponding n-gram probability may be poorly estimated, leading to a higher perplexity.\n",
        "- Laplace Smoothing Effect:\n",
        "Smoothing helps handle unseen n-grams, but when n increases, the denominator in Laplace smoothing (context_counts[context] + alpha * vocab_size) also increases, making probabilities even smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLTK Perplexity Calculation Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The NLTK uses 2**(cross_entropy for the text). This results in a different ranges, but still the perplexity increases as N increases. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
