{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading sentences and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv(\"stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\\t\")\n",
    "labels = pd.read_csv(\"stanfordSentimentTreebank/sentiment_labels.txt\", sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sentences.merge(labels, left_on=\"sentence_index\", right_on=\"phrase ids\", how=\"left\")\n",
    "data = data[[\"sentence\", \"sentiment values\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>0.37500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment values\n",
       "0  The Rock is destined to be the 21st Century 's...           0.50000\n",
       "1  The gorgeously elaborate continuation of `` Th...           0.44444\n",
       "2                     Effective but too-tepid biopic           0.50000\n",
       "3  If you sometimes like to go to the movies to h...           0.42708\n",
       "4  Emerges as something rare , an issue movie tha...           0.37500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Scores to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment(score):\n",
    "    if 0 <= score <= 0.2:\n",
    "        return 0  \n",
    "    elif 0.2 < score <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < score <= 0.6:\n",
    "        return 2\n",
    "    elif 0.6 < score <= 0.8:\n",
    "        return 3\n",
    "    elif 0.8 < score <= 1.0:\n",
    "        return 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"] = data[\"sentiment values\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment values</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.44444</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>0.42708</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment values  label\n",
       "0  The Rock is destined to be the 21st Century 's...           0.50000      2\n",
       "1  The gorgeously elaborate continuation of `` Th...           0.44444      2\n",
       "2                     Effective but too-tepid biopic           0.50000      2\n",
       "3  If you sometimes like to go to the movies to h...           0.42708      2\n",
       "4  Emerges as something rare , an issue movie tha...           0.37500      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 - Implemenation of Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Naive Bayes Classifier.\n",
    "        alpha: Smoothing parameter for Laplace smoothing.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha  # Laplace smoothing\n",
    "        self.class_priors = None  # Prior probabilities P(Class)\n",
    "        self.word_probs = None  # Likelihood P(Word | Class)\n",
    "        self.vocab = None  # Vocabulary\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the classifier using text data.\n",
    "        X: List of text samples (sentences).\n",
    "        y: Corresponding class labels.\n",
    "        \"\"\"\n",
    "        X = [preprocess(text) for text in X]\n",
    "        # Create vocabulary\n",
    "        all_words = set(word for text in X for word in text.split())\n",
    "        self.vocab = {word: i for i, word in enumerate(all_words)}\n",
    "        V = len(self.vocab)  # Vocabulary size\n",
    "        \n",
    "        # Get unique class labels\n",
    "        classes = np.unique(y)\n",
    "        num_classes = len(classes)\n",
    "        \n",
    "        # Initialize probability tables\n",
    "        self.class_priors = np.zeros(num_classes)\n",
    "        word_counts = np.zeros((num_classes, V))  # Word frequency per class\n",
    "        class_counts = np.zeros(num_classes)  # Total words per class\n",
    "        \n",
    "        # Compute class priors and word counts\n",
    "        for i, cls in enumerate(classes):\n",
    "            class_indices = np.where(y == cls)[0]\n",
    "            self.class_priors[i] = len(class_indices) / len(y)  # P(Class)\n",
    "            \n",
    "            for idx in class_indices:\n",
    "                words = X[idx].split()\n",
    "                for word in words:\n",
    "                    if word in self.vocab:\n",
    "                        word_index = self.vocab[word]\n",
    "                        word_counts[i, word_index] += 1\n",
    "                        class_counts[i] += 1\n",
    "        \n",
    "        # Apply Laplace Smoothing: P(Word | Class) = (word_count + alpha) / (total_words + alpha * V)\n",
    "        self.word_probs = (word_counts + self.alpha) / (class_counts[:, None] + self.alpha * V)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class of new text samples.\n",
    "        X: List of text samples (sentences).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        log_class_priors = np.log(self.class_priors)\n",
    "        \n",
    "        for text in X:\n",
    "            text = preprocess(text)\n",
    "            words = text.split()\n",
    "            log_probs = log_class_priors.copy()  # Initialize with log priors\n",
    "            \n",
    "            for word in words:\n",
    "                if word in self.vocab:\n",
    "                    word_index = self.vocab[word]\n",
    "                    log_probs += np.log(self.word_probs[:, word_index])\n",
    "            \n",
    "            predictions.append(np.argmax(log_probs))  # Choose class with highest log probability\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(len(data))  # Get shuffled indices\n",
    "data = data.iloc[shuffled_indices].reset_index(drop=True)  # Shuffle data\n",
    "train_size = int(0.8 * len(data))\n",
    "X_train, y_train = data[\"sentence\"][:train_size], np.array(data[\"label\"][:train_size])\n",
    "X_test, y_test = data[\"sentence\"][train_size:], np.array(data[\"label\"][train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5061\n",
      "Training samples: 9484, Test samples: 2371\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with built in Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Naïve Bayes Accuracy: 0.5065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a pipeline with CountVectorizer + MultinomialNB\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred_sklearn = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Scikit-Learn Naïve Bayes Accuracy: {accuracy_sklearn:.4f}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.vocab = None  # Stores unique bi-grams\n",
    "    \n",
    "    def extract_bigrams(self, text):\n",
    "        words = text.split()\n",
    "        #takes every two consecutive words together as a bigram\n",
    "        return [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "    \n",
    "    def fit(self, X, y): #X is a list of sentences, y is a list of labels\n",
    "\n",
    "        vocab_set = set()\n",
    "        for text in X:\n",
    "            vocab_set.update(self.extract_bigrams(text)) # store in this set all unique bi-grams\n",
    "\n",
    "        self.vocab = {bi_gram: i for i, bi_gram in enumerate(vocab_set)}#dictionary; index each unique bigram\n",
    "        V = len(self.vocab) #vocab size\n",
    "\n",
    "        # Convert sentences to feature vectors 1 if bigram is present in the sentence, 0 otherwise\n",
    "\n",
    "        X_transformed = np.zeros((len(X), V)) #initialize a matrix of zeros\n",
    "        for i, text in enumerate(X):\n",
    "            bi_grams = self.extract_bigrams(text)\n",
    "            for bi_gram in bi_grams:\n",
    "                if bi_gram in self.vocab:\n",
    "                    X_transformed[i, self.vocab[bi_gram]] = 1 #if bigram is present, set it to 1\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(V)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Training with Mini-Batch Stochastic Gradient Descent\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data each epoch\n",
    "            indices = np.arange(len(y))\n",
    "            np.random.shuffle(indices) # Shuffle indices\n",
    "            X_shuffled, y_shuffled = X_transformed[indices], y[indices]#now the data is shuffled and still with its corresponding labels\n",
    "\n",
    "            # Mini-batch updates\n",
    "            for i in range(0, len(y), self.batch_size): #Iterate over the dataset in steps of batch_size.\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "                \n",
    "                #Compute predictions\n",
    "                logits = np.dot(X_batch, self.weights) + self.bias # di el heya wx+b\n",
    "                predictions = 1 / (1 + np.exp(-logits))  # hena sigmoid activation function\n",
    "\n",
    "                # Compute gradients\n",
    "                error = predictions - y_batch # di heya predicted - actual\n",
    "                dw = np.dot(X_batch.T, error) / len(y_batch) #Derivative of the loss w.r.t. weights.\n",
    "                db = np.sum(error) / len(y_batch)  #Derivative of the loss w.r.t. bias\n",
    "                \n",
    "                #Updating weights and bias\n",
    "                self.weights -= self.learning_rate * dw #w=w-alpha*dw\n",
    "                self.bias -= self.learning_rate * db #b=b-alpha*db\n",
    "\n",
    "            # Print loss every 100 epochs\n",
    "            # if epoch % 100 == 0:\n",
    "            #     loss = -np.mean(y_batch * np.log(predictions + 1e-9) + (1 - y_batch) * np.log(1 - predictions + 1e-9))\n",
    "            #     print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for new sentences.\"\"\"\n",
    "        X_transformed = np.zeros((len(X), len(self.vocab)))\n",
    "\n",
    "        for i, text in enumerate(X):\n",
    "            bi_grams = self.extract_bigrams(text)\n",
    "            for bi_gram in bi_grams:\n",
    "                if bi_gram in self.vocab:\n",
    "                    X_transformed[i, self.vocab[bi_gram]] = 1\n",
    "\n",
    "        # Compute predictions\n",
    "        logits = np.dot(X_transformed, self.weights) + self.bias\n",
    "        probabilities = 1 / (1 + np.exp(-logits)) #sigmoid activation function\n",
    "        return (probabilities >= 0.5).astype(int)  # Return binary predictions (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression is binary classifier so we need to convert the labels to 0 and 1\n",
    "#so i did 0-1 -> 0 and 3-4 -> 1 and removed the neutral class 2\n",
    "\n",
    "data = data[data[\"label\"] != 2]  # Remove neutral class\n",
    "data[\"binary_label\"] = data[\"label\"].apply(lambda x: 1 if x > 2 else 0)\n",
    "\n",
    "# Split into features and labels\n",
    "X = data[\"sentence\"].values # Text sentences\n",
    "y = data[\"binary_label\"].values# Binary sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=0.01, epochs=100, batch_size=32)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.15%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with scikit-learn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create an instance of your logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Extract vocabulary from training data\n",
    "vocab_set = set()\n",
    "for text in X_train:\n",
    "    vocab_set.update(model.extract_bigrams(text))\n",
    "\n",
    "model.vocab = {bi_gram: i for i, bi_gram in enumerate(vocab_set)}\n",
    "\n",
    "V = len(model.vocab)  # Vocabulary size\n",
    "\n",
    "# this Function does exactly the same thing as the one in the class but yeahhhhhhh\n",
    "#had to do this so i can give the built in function the same input as the one in the class\n",
    "def transform_sentences(X, vocab):\n",
    "    X_transformed = np.zeros((len(X), len(vocab)))\n",
    "    for i, text in enumerate(X):\n",
    "        bi_grams = model.extract_bigrams(text)\n",
    "        for bi_gram in bi_grams:\n",
    "            if bi_gram in vocab:\n",
    "                X_transformed[i, vocab[bi_gram]] = 1\n",
    "    return X_transformed\n",
    "\n",
    "# Convert X_train and X_test to feature vectors\n",
    "X_train_transformed = transform_sentences(X_train, model.vocab)\n",
    "X_test_transformed = transform_sentences(X_test, model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Logistic Regression Accuracy: 51.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train scikit-learn's logistic regression\n",
    "sklearn_lr = LogisticRegression(solver=\"lbfgs\", max_iter=100)\n",
    "sklearn_lr.fit(X_train_transformed, y_train)  # Use transformed features\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sklearn = sklearn_lr.predict(X_test_transformed)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Scikit-Learn Logistic Regression Accuracy: {accuracy_sklearn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with scikit-learn SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Accuracy: 52.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Initialize and train SGDClassifier as logistic regression\n",
    "sgd_lr = SGDClassifier(loss=\"log_loss\", max_iter=100, learning_rate=\"optimal\")\n",
    "sgd_lr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sgd = sgd_lr.predict(X_test_transformed)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "print(f\"SGDClassifier Accuracy: {accuracy_sgd * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
